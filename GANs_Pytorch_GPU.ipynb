{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GANs_Pytorch_GPU.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBG0uZCnO7n5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rG2ItWPO9uE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf.gfile.MkDir('data')\n",
        "!mkdir data\n",
        "!mkdir results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4Entci4ORHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Deep Convolutional GANs\n",
        "\n",
        "# Importing the libraries\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fviogLa0Q2xa",
        "colab_type": "code",
        "outputId": "227f917b-88ad-48cc-8462-a6a0bceda5d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "use_cuda = True # True if you want to use CUDA otherwise False\n",
        "\n",
        "cuda_available = False\n",
        "device = torch.cuda.get_device_name(0)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "  cuda_available = True\n",
        "  print('CUDA is available')\n",
        "else:\n",
        "  print('CUDA not available')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TYF9ey6Qzpd",
        "colab_type": "code",
        "outputId": "dadeb18f-ffaf-40cc-ef62-2ae6aa9ed156",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "\n",
        "# Setting some hyperparameters\n",
        "batchSize = 64 # We set the size of the batch.\n",
        "imageSize = 64 # We set the size of the generated images (64x64).\n",
        "\n",
        "# Creating the transformations\n",
        "transform = transforms.Compose([transforms.Scale(imageSize), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]) # We create a list of transformations (scaling, tensor conversion, normalization) to apply to the input images.\n",
        "\n",
        "# Loading the dataset\n",
        "dataset = dset.CIFAR10(root = './data', download = True, transform = transform) # We download the training set in the ./data folder and we apply the previous transformations on each image.\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2) # We use dataLoader to get the images of the training set batch by batch.\n",
        "\n",
        "# Defining the weights_init function that takes as input a neural network m and that will initialize all its weights.\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "# Defining the generator\n",
        "\n",
        "class G(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(G, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        return output\n",
        "\n",
        "# Creating the generator\n",
        "netG = G()\n",
        "netG.apply(weights_init)\n",
        "\n",
        "if cuda_available:\n",
        "  netG = netG.cuda()\n",
        "  print('netG is cuda')\n",
        "\n",
        "# Defining the discriminator\n",
        "\n",
        "class D(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(D, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 4, 2, 1, bias = False),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias = False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace = True),\n",
        "            nn.Conv2d(512, 1, 4, 1, 0, bias = False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.main(input)\n",
        "        return output.view(-1)\n",
        "\n",
        "# Creating the discriminator\n",
        "netD = D()\n",
        "netD.apply(weights_init)\n",
        "\n",
        "if cuda_available:\n",
        "  netD.cuda()\n",
        "  print('netD is cuda')\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:209: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "netG is cuda\n",
            "netD is cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPN1Z_Z8Qp5l",
        "colab_type": "code",
        "outputId": "7a83ccd6-16c4-41d1-ac3b-567774d865c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2465
        }
      },
      "source": [
        "import time\n",
        "\n",
        "# Training the DCGANs\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "    \n",
        "    istart_time = time.time()\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        \n",
        "        # 1st Step: Updating the weights of the neural network of the discriminator\n",
        "\n",
        "        netD.zero_grad()\n",
        "        \n",
        "        # 1.1 Training the discriminator with a real image of the dataset\n",
        "        real, _ = data\n",
        "        input = Variable(real)\n",
        "        target = Variable(torch.ones(input.size()[0]))\n",
        "        \n",
        "        if cuda_available:\n",
        "          input = input.cuda()\n",
        "          target = target.cuda()\n",
        "        \n",
        "        output = netD(input)\n",
        "        errD_real = criterion(output, target)\n",
        "        \n",
        "        # 1.2 Training the discriminator with a fake image generated by the generator\n",
        "        noise = Variable(torch.randn(input.size()[0], 100, 1, 1))\n",
        "        target = Variable(torch.zeros(input.size()[0]))\n",
        "        \n",
        "        \n",
        "        if cuda_available:\n",
        "          noise = noise.cuda()\n",
        "          fake = netG(noise)\n",
        "          fake = fake.cuda()\n",
        "          target = target.cuda()\n",
        "        else:\n",
        "          fake = netG(noise)\n",
        "        \n",
        "        \n",
        "        output = netD(fake.detach())\n",
        "        errD_fake = criterion(output, target)\n",
        "        \n",
        "        # 1.3 Backpropagating the total error\n",
        "        errD = errD_real + errD_fake\n",
        "        errD.backward()\n",
        "        optimizerD.step()\n",
        "\n",
        "        # 2nd Step: Updating the weights of the neural network of the generator\n",
        "\n",
        "        netG.zero_grad()\n",
        "        target = Variable(torch.ones(input.size()[0]))\n",
        "        \n",
        "        if cuda_available:\n",
        "          target = target.cuda()\n",
        "          \n",
        "        output = netD(fake)\n",
        "        errG = criterion(output, target)\n",
        "        errG.backward()\n",
        "        optimizerG.step()\n",
        "        \n",
        "        # 3rd Step: Printing the losses and saving the real images and the generated images of the minibatch every 100 steps\n",
        "\n",
        "        \n",
        "        if i % 10 == 0:\n",
        "            iend_time = time.time()\n",
        "            print('10 processing took: '+str(iend_time - istart_time))  \n",
        "            istart_time = time.time()\n",
        "            \n",
        "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, epochs, i, len(dataloader), errD.data, errG.data))\n",
        "            print('==============')\n",
        "            vutils.save_image(real, '%s/real_samples.png' % \"./results\", normalize = True)\n",
        "            fake = netG(noise)\n",
        "            vutils.save_image(fake.data, '%s/fake_samples_i_%03d.png' % (\"./results\", epoch), normalize = True)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    print('Epoch: '+epoch+' took: '+str(end_time - start_time))  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 processing took: 0.33267784118652344\n",
            "[0/10][0/782] Loss_D: 1.6861 Loss_G: 7.0251\n",
            "==============\n",
            "10 processing took: 2.2218921184539795\n",
            "[0/10][10/782] Loss_D: 1.2998 Loss_G: 11.0887\n",
            "==============\n",
            "10 processing took: 2.162893295288086\n",
            "[0/10][20/782] Loss_D: 0.2288 Loss_G: 9.2538\n",
            "==============\n",
            "10 processing took: 2.15584135055542\n",
            "[0/10][30/782] Loss_D: 0.5362 Loss_G: 18.0878\n",
            "==============\n",
            "10 processing took: 2.1461341381073\n",
            "[0/10][40/782] Loss_D: 0.3139 Loss_G: 24.4209\n",
            "==============\n",
            "10 processing took: 2.1541285514831543\n",
            "[0/10][50/782] Loss_D: 0.6218 Loss_G: 27.4326\n",
            "==============\n",
            "10 processing took: 2.15041184425354\n",
            "[0/10][60/782] Loss_D: 0.1236 Loss_G: 7.5171\n",
            "==============\n",
            "10 processing took: 2.1684958934783936\n",
            "[0/10][70/782] Loss_D: 0.1642 Loss_G: 9.8678\n",
            "==============\n",
            "10 processing took: 2.1568715572357178\n",
            "[0/10][80/782] Loss_D: 0.3764 Loss_G: 6.2947\n",
            "==============\n",
            "10 processing took: 2.154702663421631\n",
            "[0/10][90/782] Loss_D: 0.2038 Loss_G: 6.1569\n",
            "==============\n",
            "10 processing took: 2.1623799800872803\n",
            "[0/10][100/782] Loss_D: 0.2165 Loss_G: 4.6030\n",
            "==============\n",
            "10 processing took: 2.1605687141418457\n",
            "[0/10][110/782] Loss_D: 0.2091 Loss_G: 4.4105\n",
            "==============\n",
            "10 processing took: 2.173497438430786\n",
            "[0/10][120/782] Loss_D: 0.5394 Loss_G: 2.5247\n",
            "==============\n",
            "10 processing took: 2.164090871810913\n",
            "[0/10][130/782] Loss_D: 4.1209 Loss_G: 7.2765\n",
            "==============\n",
            "10 processing took: 2.154618740081787\n",
            "[0/10][140/782] Loss_D: 0.4811 Loss_G: 3.9744\n",
            "==============\n",
            "10 processing took: 2.164649486541748\n",
            "[0/10][150/782] Loss_D: 1.6268 Loss_G: 1.9989\n",
            "==============\n",
            "10 processing took: 2.1666030883789062\n",
            "[0/10][160/782] Loss_D: 0.2971 Loss_G: 4.5579\n",
            "==============\n",
            "10 processing took: 2.1574971675872803\n",
            "[0/10][170/782] Loss_D: 0.3068 Loss_G: 4.9816\n",
            "==============\n",
            "10 processing took: 2.1641180515289307\n",
            "[0/10][180/782] Loss_D: 0.3993 Loss_G: 6.4109\n",
            "==============\n",
            "10 processing took: 2.1608479022979736\n",
            "[0/10][190/782] Loss_D: 0.1453 Loss_G: 5.6284\n",
            "==============\n",
            "10 processing took: 2.162626266479492\n",
            "[0/10][200/782] Loss_D: 0.1764 Loss_G: 5.2027\n",
            "==============\n",
            "10 processing took: 2.169253349304199\n",
            "[0/10][210/782] Loss_D: 0.1157 Loss_G: 6.1140\n",
            "==============\n",
            "10 processing took: 2.172398805618286\n",
            "[0/10][220/782] Loss_D: 0.2673 Loss_G: 9.5323\n",
            "==============\n",
            "10 processing took: 2.1691184043884277\n",
            "[0/10][230/782] Loss_D: 0.2499 Loss_G: 12.4023\n",
            "==============\n",
            "10 processing took: 2.173011064529419\n",
            "[0/10][240/782] Loss_D: 0.5895 Loss_G: 12.0922\n",
            "==============\n",
            "10 processing took: 2.166053295135498\n",
            "[0/10][250/782] Loss_D: 0.2307 Loss_G: 9.3708\n",
            "==============\n",
            "10 processing took: 2.1698901653289795\n",
            "[0/10][260/782] Loss_D: 0.0657 Loss_G: 5.3776\n",
            "==============\n",
            "10 processing took: 2.1724131107330322\n",
            "[0/10][270/782] Loss_D: 0.2584 Loss_G: 7.9404\n",
            "==============\n",
            "10 processing took: 2.172865152359009\n",
            "[0/10][280/782] Loss_D: 1.2580 Loss_G: 13.9591\n",
            "==============\n",
            "10 processing took: 2.163045883178711\n",
            "[0/10][290/782] Loss_D: 0.4832 Loss_G: 5.4472\n",
            "==============\n",
            "10 processing took: 2.169464588165283\n",
            "[0/10][300/782] Loss_D: 1.0698 Loss_G: 1.7716\n",
            "==============\n",
            "10 processing took: 2.1746110916137695\n",
            "[0/10][310/782] Loss_D: 0.6588 Loss_G: 4.9203\n",
            "==============\n",
            "10 processing took: 2.1573753356933594\n",
            "[0/10][320/782] Loss_D: 0.6480 Loss_G: 4.5881\n",
            "==============\n",
            "10 processing took: 2.167912244796753\n",
            "[0/10][330/782] Loss_D: 0.8907 Loss_G: 6.3997\n",
            "==============\n",
            "10 processing took: 2.1639063358306885\n",
            "[0/10][340/782] Loss_D: 0.4081 Loss_G: 3.7475\n",
            "==============\n",
            "10 processing took: 2.171308994293213\n",
            "[0/10][350/782] Loss_D: 1.6055 Loss_G: 2.6914\n",
            "==============\n",
            "10 processing took: 2.172783613204956\n",
            "[0/10][360/782] Loss_D: 0.7098 Loss_G: 3.9657\n",
            "==============\n",
            "10 processing took: 2.1554925441741943\n",
            "[0/10][370/782] Loss_D: 0.6270 Loss_G: 5.8622\n",
            "==============\n",
            "10 processing took: 2.1749343872070312\n",
            "[0/10][380/782] Loss_D: 0.5951 Loss_G: 3.2621\n",
            "==============\n",
            "10 processing took: 2.177391767501831\n",
            "[0/10][390/782] Loss_D: 0.2757 Loss_G: 4.6819\n",
            "==============\n",
            "10 processing took: 2.1705219745635986\n",
            "[0/10][400/782] Loss_D: 0.3730 Loss_G: 7.8553\n",
            "==============\n",
            "10 processing took: 2.1744585037231445\n",
            "[0/10][410/782] Loss_D: 0.7706 Loss_G: 2.8098\n",
            "==============\n",
            "10 processing took: 2.1741509437561035\n",
            "[0/10][420/782] Loss_D: 0.6822 Loss_G: 3.8222\n",
            "==============\n",
            "10 processing took: 2.168727397918701\n",
            "[0/10][430/782] Loss_D: 0.4471 Loss_G: 3.1533\n",
            "==============\n",
            "10 processing took: 2.1708755493164062\n",
            "[0/10][440/782] Loss_D: 0.3673 Loss_G: 5.9611\n",
            "==============\n",
            "10 processing took: 2.1713714599609375\n",
            "[0/10][450/782] Loss_D: 0.3035 Loss_G: 4.0853\n",
            "==============\n",
            "10 processing took: 2.179635763168335\n",
            "[0/10][460/782] Loss_D: 1.0380 Loss_G: 4.5263\n",
            "==============\n",
            "10 processing took: 2.180680513381958\n",
            "[0/10][470/782] Loss_D: 0.1845 Loss_G: 5.3261\n",
            "==============\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGk8db_PPgiJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}